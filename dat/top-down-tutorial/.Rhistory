i[,'a']
nrow(i)
i[1,'a'] <- 1
i
beepr::beep(8)
catalogue <- wopr::getCatalogue()
wopr::downloadData(catalogue, 'e:/wopr')
# load package
library(wopr, lib='c:/research/r/library')
shiny::runApp('wpgp/wopr/inst/woprVision')
shiny::runApp('wpgp/wopr/inst/woprVision')
wopr:::woprVision_global$version_info
# load package
library(wopr, lib='c:/research/r/library')
runApp('wpgp/wopr/inst/woprVision')
shiny::runApp('wpgp/wopr/inst/woprVision')
# load package
library(wopr, lib='c:/research/r/library')
wopr_dir
rm(wopr_dir)
runApp('wpgp/wopr/inst/woprVision')
hist(rgamma(1e3,1,1))
hist(rgamma(1e3,0.1,0.1))
hist(rgamma(1e3,1,1))
hist(rnorm(1e3,0,1), xlim=c(0,10))
as.numeric('1000L')
1000L
n=1000
nL
as.long(1000)
as.integer(1000)
as.integer(10000)
library(wopr,'c:/research/r/library')
catalogue <- wopr::getCatalogue()
catalogue <- wopr::getCatalogue()
catalogue <- wopr::getCatalogue()
head(catalogue)
?wopr::downloadData
wopr::downloadData(dat = catalogue,
wopr_dir = 'E:/wopr',
maxsize = 100)
wopr::downloadData(dat = catalogue,
wopr_dir = 'E:/wopr',
maxsize = 1000)
wopr::downloadData(dat = catalogue,
wopr_dir = 'E:/wopr',
maxsize = 1000)
wopr::downloadData(dat = catalogue,
wopr_dir = 'E:/wopr',
maxsize = 2000)
wopr::downloadData(dat = catalogue,
wopr_dir = 'E:/wopr',
maxsize = 2000)
wopr::downloadData(dat = catalogue,
wopr_dir = 'E:/wopr',
maxsize = 2000)
x <- sf::st_read('c:/research/tmp/National EAS/National_EAS.shp')
x <- sf::st_read('c:/research/tmp/National EAS/National__EAS.shp')
names(x)
head(x)
sf::st_write(x, 'c:/research/tmp/National EAS/SLE_National_EAS.geojson')
sqrt(5000)
sqrt(10000)
fname <- 'AGO_buildings_v1_1_imagery_year.tif'
fname <- 'e:/research/AGO_buildings_v1_1_imagery_year.tif'
year <- raster::raster(fname)
fname <- 'e:/peanutButter/AGO_buildings_v1_1_imagery_year.tif'
year <- raster::raster(fname)
table(year[])
fname <- 'e:/peanutButter/BDI_buildings_v1_1_imagery_year.tif'
year <- raster::raster(fname)
table(year[])
srcdir <- 'E:/peanutButter'
peanutButter::jelly(srcdir)
# load
library(peanutButter, lib='c:/research/r/library')
srcdir <- 'E:/peanutButter'
peanutButter::jelly(srcdir)
dat = read.csv('out/tab/data.csv', stringsAsFactors=F)
# working directory
setwd(file.path(dirname(rstudioapi::getSourceEditorContext()$path),'../wd'))
names(NA)
NA %in% names(NA)
any(NA %in% names(NA))
bookdown::serve_book()
bookdown::serve_book()
bookdown::serve_book()
library(randomForest)
?tuneRF
?randomForest
#-----------------------------------#
# packages
library(randomForest) # estimating random forest model
# working directory
local <- T
if(local){
setwd(file.path(dirname(rstudioapi::getSourceEditorContext()$path),'dat/top-down-tutorial'))
} else {
setwd("//worldpop.files.soton.ac.uk/worldpop/Projects/WP517763_GRID3/Working/git/top-down-tutorial")
}
#--
# training data from municipalities
master_train <- read.csv("master_train.csv")
head(master_train[,1:5]) # only showing first five columns
# covariates from enumeration areas
master_predict <- read.csv("master_predict.csv")
head(master_predict[,1:4]) # only showing first four columns
master_predict[1,'EA_id']
as.integer(master_predict[1,'EA_id'])
# response
y_data <- master_train$pop / master_train$area
y_data <- log(y_data)
# histogram of response
hist(y_data, main=NA, xlab="log(population_density)")
# covariate names (i.e. column names)
cols <- colnames(master_train)
print(cols)
# select column names that contain the word 'mean'
cov_names <- cols[grepl('mean', cols)]
print(cov_names)
# subset the data.frame to only these columns
x_data <- master_train[,cov_names]
head(x_data[,1:2]) # only showing first two columns
# model fitting
popfit <- tuneRF(x=x_data,
y=y_data,
plot=TRUE,
mtryStart=length(x_data)/3,
ntreeTry=500,
improve=0.0001, # threshold on the OOB error to continue the search
stepFactor=1.20, # incremental improvement of mtry
trace=TRUE,
doBest=TRUE, # last model trained with the best mtry
nodesize=length(y_data)/1000,
na.action=na.omit,
importance=TRUE,
sampsize=length(y_data), # size of the sample to draw for OOB
replace=TRUE) # sample with replacement
names(popfit)
popfit$mtry
getwd()
?randomForest
class(popfit)
names(popfit)
popfit$mtry
# save model
save(popfit, file='popfit.Rdata')
# random forest predictions
master_predict$predicted <- predict(popfit,
newdata = master_predict)
# back-transform predictions to natural scale
master_predict$predicted_exp <- exp(master_predict$predicted)
# sum exponentiated predictions among EAs in each municipality
predicted_exp_sum <- aggregate(master_predict$predicted_exp,
by = list(geo_code=master_predict$geo_code),
FUN = sum)
# modify column names
names(predicted_exp_sum) <- c('geo_code','predicted_exp_sum')
# merge predicted_exp_sum into master_train based on geo_code
master_predict <- merge(master_predict,
predicted_exp_sum,
by='geo_code')
# merge municipality total populations from master_train into master_predict
master_predict <- merge(master_predict,
master_train[,c('geo_code','pop')],
by = 'geo_code')
# modify column name
names(master_predict)[ncol(master_predict)] <- 'pop_municipality'
# calculate EA-level population estimates
master_predict$predicted_pop <- with(master_predict, predicted_exp / predicted_exp_sum * pop_municipality)
# sum EA population estimates within each municipality
test <- aggregate(master_predict$predicted_pop,
by = list(geo_code=master_predict$geo_code),
FUN = sum)
# modify column names
names(test) <- c('geo_code','predicted_pop')
# merge municipality population totals
test <- merge(test,
master_train[,c('geo_code','pop')],
by = 'geo_code')
# test if estimates match muncipality population totals
all(round(test$pop) == round(test$predicted_pop))
# goodness-of-fit metrics
print(popfit)
# plot observed vs predicted (out-of-bag)
plot(x = y_data,
y = predict(popfit),
main = 'Observed vs Predicted log-Densities')
# 1:1 line
abline(a=0, b=1, col='red')
# plot residuals (out-of-bag)
plot(x = predict(popfit),
y = y_data - predict(popfit),
main = 'Residuals vs Predicted',
ylab = 'Out-of-bag residuals',
xlab = 'Out-of-bag prediction')
# horizontal line at zero
abline(h=0, col='red')
layout(matrix(1:2, nrow=1))
for(cov_name in c('mean.bra_srtm_slope_100m', 'mean.bra_viirs_100m_2016')){
# combine EA-level and municipality-level values into a single vector
y <- c(master_predict[,cov_name], master_train[,cov_name])
# create corresponding vector identifying spatial scale
x <- c(rep('enumeration_area', nrow(master_predict)),
rep('municipality', nrow(master_train)))
# create boxplot
boxplot(y~x, xlab='Spatial Scale', ylab=cov_name)
}
library('sf')
sf_polygons <- st_read('../../wd/in/censusEAs/BR_Setores_2019.shp')
sf_polygons$EA_id <- 1:nrow(sf_polygons)
sf_polygons <- merge(sf_polygons,
master_predict,
by='EA_id')
library(raster)
library(exactextractr)
raster_covariate <- raster('../../wd/in/covariates/bra_viirs_100m_2016.tif')
sf_polygons$mean.bra_viirs_100m_2016 <- exact_extract(x = raster_covariate,
y = sf_polygons,
fun = 'mean')
head(sf_polygons$mean.bra_viirs_100m_2016)
sf_polygons
sf_polygons <- st_read('../../wd/in/censusEAs/BR_Setores_2019.shp')
names(sf_polygons)
master_predict$EA_id <- 1:nrow(master_predict)
sf_polygons <- st_read('../../wd/in/censusEAs/BR_Setores_2019.shp')
sf_polygons$EA_id <- 1:nrow(sf_polygons)
sf_polygons <- merge(sf_polygons,
master_predict,
by='EA_id')
sf_polygons
tail(master_predict)
tail(sf_polygons)
raster_covariate <- raster('../../wd/in/covariates/bra_viirs_100m_2016.tif')
sf_polygons$mean.bra_viirs_100m_2016 <- exact_extract(x = raster_covariate,
y = sf_polygons,
fun = 'mean')
sf_polygons
?exact_extract
mastergrid <- raster('../../wd/in/bra_level0_100m_2000_2020.tif')
cells <- which(!is.na(mastergrid[1:1e8]))
cells <- which(!is.na(mastergrid[1:1e7]))
mastergrid_predict <- data.frame(row.names = cells)
raster_covariate <- raster('../../wd/in/covariates/bra_viirs_100m_2016.tif')
mastergrid_predict$bra_viirs_100m_2016 <- raster_covariate[cells]
xy <- xyFromCell(mastergrid, cells)
mastergrid_predict$bra_viirs_100m_2016_alt <- extract(raster_covariate, xy)
mastergrid_predict$predicted_pop <- runif(nrow(mastergrid_predict), 0, 1000)
raster_predict <- raster(mastergrid)
raster_predict[cells] <- mastergrid_predict[cells, 'predicted_pop']
mastergrid_predict[cells, 'bra_viirs_100m_2016'] <- raster_covariate[cells]
raster_covariate <- raster('../../wd/in/covariates/bra_viirs_100m_2016.tif')
mastergrid_predict[cells, 'bra_viirs_100m_2016'] <- raster_covariate[cells]
xy <- xyFromCell(mastergrid, cells)
mastergrid_predict[cells, 'bra_viirs_100m_2016_alt'] <- extract(raster_covariate, xy)
rm(list=ls()); gc(); cat("\014"); try(dev.off(), silent=T)
try(source(file.path(dirname(rstudioapi::getSourceEditorContext()$path),'0. setup.R')))
# 1. Set-up ---------------------------------------------------------------
# working directory
local <- T
if(local){
setwd(file.path(dirname(rstudioapi::getSourceEditorContext()$path),'wd'))
} else {
setwd("//worldpop.files.soton.ac.uk/worldpop/Projects/WP517763_GRID3/Working/git/top-down-tutorial")
}
dir.create('in', showWarnings=F)
dir.create('out', showWarnings=F)
data_path <- file.path(getwd(), "in")
output_path <- file.path(getwd(), "out")
# copy source data
copyWP(srcdir = 'E:/WorldPop/Projects/WP517763_GRID3/Working/git/top-down-tutorial/in',
outdir = data_path,
local=T)
# load packages
library(sf) # manipulating vector GIS file
library(raster) # manipulating raster GIS file
library(fasterize) # fast rasterization
library(tictoc) # compute running time
library(tidyverse) # manipulating dataframes
library(exactextractr) # fast zonal statistics
library(data.table) # fast dataframe writing
##-- load data --##
# Boundaries from IGBE 2019
admin3_poly <- st_read(file.path(data_path, 'adminBoundaries/admin_municipality.gpkg'), stringsAsFactors = F)
# Mastergrid from WorldPop
masterGrid <- raster(file.path(data_path, "masterGrid.tif"))
# Population data from IGBE projections 2020
# https://biblioteca.ibge.gov.br/index.php/biblioteca-catalogo?view=detalhes&id=293420
pop_admin3 <- read.csv(file.path(data_path, 'population/pop_projections_2020_municipality.csv'))
# EA boundaries
# IBGE census EAs(public)
# http://geoftp.ibge.gov.br/organizacao_do_territorio/malhas_territoriais/malhas_de_setores_censitarios__divisoes_intramunicipais/2019/Malha_de_setores_(shp)_Brasil/
EA_poly <- st_read(file.path(data_path, 'censusEAs/BR_Setores_2019.shp'), stringsAsFactors = F)
EA_poly$EA_id <- 1:nrow(EA_poly)
# 2. Create covariates dataset  ---------------------------------------------------
# Create a raster stack of all covariates
raster_names <- list.files(file.path(data_path, "covariates/"), pattern="tif$", full.names = T)
raster_stack <- stack(raster_names) # 10 rasters
# Extract zonal statistics for every EA
tic() # 1h
cov_EA <- exact_extract(raster_stack, EA_poly, fun='mean', progress=T, force_df=T, stack_apply=T)
toc()
cov_EA$EA_id <- EA_poly$EA_id # EA_poly$CD_SETOR
# Extract zonal statistics for every municipality
tic() # 22 min
cov_admin3 <- exact_extract(raster_stack, admin3_poly, fun='mean', progress=T, force_df=T, stack_apply=T)
toc()
cov_admin3$geo_code <- admin3_poly$CD_GEOCODI
1891/60
1035.7/60
# area of each municipality (admin3)
admin3_poly$area <- st_area(admin3_poly) # m2
# create data.frame
master_train <- cov_admin3 %>%
left_join(
admin3_poly %>%
st_drop_geometry() %>%
rename(geo_code = CD_GEOCODI) %>%
select(geo_code, area)
) %>%
right_join(pop_admin3 %>%
mutate(
geo_code = as.character(geo_code)
))
# 4. Create predicting dataset --------------------------------------------
# Find corresponding admin 3
admin3_poly <- admin3_poly %>%
mutate(
CD_GEOCODI = as.integer(CD_GEOCODI)
)
admin3_raster <- fasterize(admin3_poly, masterGrid, field= 'CD_GEOCODI')
tic() # 6 min
EA_admin3 <- exact_extract(admin3_raster, EA_poly, fun='mode', force_df=T)
toc()
# Join covariates value
EA_admin3$EA_id <- EA_poly$EA_id
master_predict <- EA_admin3 %>%
mutate(geo_code = as.character(mode)) %>%
select(-mode) %>%
right_join(
cov_EA
)
apply(master_predict,2, function(x) sum(is.na(x)))
# 46 EAs are not assigned to an admin3
# This is due to:
# 1. tiny islands that were in the gridEZ baseline and not on the NSo boundaries dataset
# 2. Tiny outputs from gridEZ algorithm
# only keep rows with no NAs
master_predict <- master_predict[apply(master_predict,1,function(x) sum(is.na(x)) == 0), ]
# Overcome issues in admin3 not present in EA dataset
admin3_withPop <- master_train %>%
select(geo_code) %>%
mutate(
admin3_municip =T
) %>%
left_join(
master_predict %>%
group_by(geo_code) %>%
summarise(admin3_ea=T)
)
master_train <- master_train %>%
left_join(
admin3_withPop
) %>%
filter(!is.na(admin3_municip)&!is.na(admin3_ea)) %>%
select(-starts_with('admin'))
x <- c('name_muni','geo_code','pop','area')
x <- c(x, sort(names(master_train)[!names(master_train) %in% x], decreasing=T))
fwrite(master_train[,x], file.path(output_path, "master_train.csv"))
x <- c('EA_id','geo_code')
x <- c(x, sort(names(master_predict)[!names(master_predict) %in% x], decreasing=T))
fwrite(master_predict[,x], file.path(output_path, "master_predict.csv"))
if(local){
input_path <- '../wd/out/'
} else {
input_path <- "//worldpop.files.soton.ac.uk/worldpop/Projects/WP517763_GRID3/Working/git/top-down-tutorial/out"
}
getwd()
file.copy(from = c(file.path(input_path, 'master_train.csv'), file.path(input_path, 'master_predict.csv')),
to = c('../dat/top-down-tutorial/master_train.csv','../dat/top-down-tutorial/master_predict.csv'),
overwrite = T)
# packages
library(randomForest) # estimating random forest model
# working directory
local <- T
if(local){
setwd(file.path(dirname(rstudioapi::getSourceEditorContext()$path),'dat/top-down-tutorial'))
} else {
setwd("//worldpop.files.soton.ac.uk/worldpop/Projects/WP517763_GRID3/Working/git/top-down-tutorial")
}
#--
# training data from municipalities
master_train <- read.csv("master_train.csv")
head(master_train[,1:5]) # only showing first five columns
# covariates from enumeration areas
master_predict <- read.csv("master_predict.csv")
head(master_predict[,1:4]) # only showing first four columns
# response
y_data <- master_train$pop / master_train$area
y_data <- log(y_data)
# histogram of response
hist(y_data, main=NA, xlab="log(population_density)")
# covariate names (i.e. column names)
cols <- colnames(master_train)
print(cols)
# select column names that contain the word 'mean'
cov_names <- cols[grepl('mean', cols)]
print(cov_names)
# subset the data.frame to only these columns
x_data <- master_train[,cov_names]
head(x_data[,1:2]) # only showing first two columns
# model fitting
popfit <- tuneRF(x=x_data,
y=y_data,
plot=TRUE,
mtryStart=length(x_data)/3,
ntreeTry=500,
improve=0.0001, # threshold on the OOB error to continue the search
stepFactor=1.20, # incremental improvement of mtry
trace=TRUE,
doBest=TRUE, # last model trained with the best mtry
nodesize=length(y_data)/1000,
na.action=na.omit,
importance=TRUE,
sampsize=length(y_data), # size of the sample to draw for OOB
replace=TRUE) # sample with replacement
#--
names(popfit)
#--
popfit$mtry
#--
# save model
save(popfit, file='popfit.Rdata')
#--
# load model
load('popfit.Rdata')
#--
# random forest predictions
master_predict$predicted <- predict(popfit,
newdata = master_predict)
# back-transform predictions to natural scale
master_predict$predicted_exp <- exp(master_predict$predicted)
# sum exponentiated predictions among EAs in each municipality
predicted_exp_sum <- aggregate(master_predict$predicted_exp,
by = list(geo_code=master_predict$geo_code),
FUN = sum)
# modify column names
names(predicted_exp_sum) <- c('geo_code','predicted_exp_sum')
# merge predicted_exp_sum into master_train based on geo_code
master_predict <- merge(master_predict,
predicted_exp_sum,
by='geo_code')
# merge municipality total populations from master_train into master_predict
master_predict <- merge(master_predict,
master_train[,c('geo_code','pop')],
by = 'geo_code')
# modify column name
names(master_predict)[ncol(master_predict)] <- 'pop_municipality'
# calculate EA-level population estimates
master_predict$predicted_pop <- with(master_predict, predicted_exp / predicted_exp_sum * pop_municipality)
# sum EA population estimates within each municipality
test <- aggregate(master_predict$predicted_pop,
by = list(geo_code=master_predict$geo_code),
FUN = sum)
# modify column names
names(test) <- c('geo_code','predicted_pop')
# merge municipality population totals
test <- merge(test,
master_train[,c('geo_code','pop')],
by = 'geo_code')
# test if estimates match muncipality population totals
all(round(test$pop) == round(test$predicted_pop))
# goodness-of-fit metrics
print(popfit)
# plot observed vs predicted (out-of-bag)
plot(x = y_data,
y = predict(popfit),
main = 'Observed vs Predicted log-Densities')
# 1:1 line
abline(a=0, b=1, col='red')
# plot residuals (out-of-bag)
plot(x = predict(popfit),
y = y_data - predict(popfit),
main = 'Residuals vs Predicted',
ylab = 'Out-of-bag residuals',
xlab = 'Out-of-bag prediction')
# horizontal line at zero
abline(h=0, col='red')
layout(matrix(1:2, nrow=1))
for(cov_name in c('mean.bra_srtm_slope_100m', 'mean.bra_viirs_100m_2016')){
# combine EA-level and municipality-level values into a single vector
y <- c(master_predict[,cov_name], master_train[,cov_name])
# create corresponding vector identifying spatial scale
x <- c(rep('enumeration_area', nrow(master_predict)),
rep('municipality', nrow(master_train)))
# create boxplot
boxplot(y~x, xlab='Spatial Scale', ylab=cov_name)
}
library('sf')
sf_polygons <- st_read('../../wd/in/censusEAs/BR_Setores_2019.shp')
sf_polygons$EA_id <- 1:nrow(sf_polygons)
sf_polygons <- merge(sf_polygons,
master_predict,
by='EA_id')
library(raster)
library(exactextractr)
raster_covariate <- raster('../../wd/in/covariates/bra_viirs_100m_2016.tif')
sf_polygons$mean.bra_viirs_100m_2016 <- exact_extract(x = raster_covariate,
y = sf_polygons,
fun = 'mean')
crs(raster_covariate)
?raster::raster
